use std::sync::{Arc, OnceLock};

use async_trait::async_trait;
use fastcrypto::hash::HashFunction;
use fleek_blake3 as blake3;
use lightning_interfaces::prelude::*;
use lightning_interfaces::types::{Block, Epoch, Event, Metadata, NodeIndex, TransactionRequest};
use lightning_interfaces::ExecutionEngineSocket;
use lightning_utils::application::QueryRunnerExt;
use narwhal_crypto::DefaultHashFunction;
use narwhal_executor::ExecutionState;
use narwhal_types::{Batch, BatchDigest, ConsensusOutput, Transaction};
use serde::{Deserialize, Serialize};
use tokio::sync::{mpsc, Notify};
use tracing::{error, info};

pub type Digest = [u8; 32];

#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct AuthenticStampedParcel {
    pub transactions: Vec<Transaction>,
    pub last_executed: Digest,
    pub epoch: Epoch,
    pub sub_dag_index: u64,
}

impl ToDigest for AuthenticStampedParcel {
    fn transcript(&self) -> TranscriptBuilder {
        panic!("We don't need this here");
    }

    fn to_digest(&self) -> Digest {
        let batch_digest =
            BatchDigest::new(DefaultHashFunction::digest_iterator(self.transactions.iter()).into());

        let mut bytes = Vec::new();
        bytes.extend_from_slice(&(self.transactions.len() as u32).to_le_bytes());
        bytes.extend_from_slice(&batch_digest.0);
        bytes.extend_from_slice(&self.last_executed);

        blake3::hash(&bytes).into()
    }
}

/// A message an authority sends out attest that an Authentic stamp parcel is accurate. When an edge
/// node gets 2f+1 of these it commits the transactions in the parcel
#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct CommitteeAttestation {
    /// The digest we are attesting is correct
    pub digest: Digest,
    /// We send random bytes with this message so it gives it a unique hash and differentiates it
    /// from the other committee members attestation broadcasts
    pub node_index: NodeIndex,
    pub epoch: Epoch,
}

pub struct Execution<Q: SyncQueryRunnerInterface, NE: Emitter> {
    /// Managing certificates generated by narwhal.
    executor: ExecutionEngineSocket,
    /// Used to signal internal consensus processes that it is time to reconfigure for a new epoch
    reconfigure_notify: Arc<Notify>,
    /// Used to send payloads to the edge node consensus to broadcast out to other nodes
    tx_narwhal_batches: mpsc::Sender<(AuthenticStampedParcel, bool)>,
    /// Query runner to check application state, mainly used to make sure the last executed block
    /// is up to date from time we were an edge node
    query_runner: Q,
    /// Notifications emitter
    notifier: NE,
    /// Send the event to the RPC
    event_tx: OnceLock<mpsc::Sender<Vec<Event>>>,
}

impl<Q: SyncQueryRunnerInterface, NE: Emitter> Execution<Q, NE> {
    pub fn new(
        executor: ExecutionEngineSocket,
        reconfigure_notify: Arc<Notify>,
        tx_narwhal_batches: mpsc::Sender<(AuthenticStampedParcel, bool)>,
        query_runner: Q,
        notifier: NE,
    ) -> Self {
        Self {
            executor,
            reconfigure_notify,
            tx_narwhal_batches,
            query_runner,
            notifier,
            event_tx: OnceLock::new(),
        }
    }

    // Returns true if the epoch changed
    pub(crate) async fn submit_batch(
        &self,
        payload: Vec<Transaction>,
        digest: Digest,
        sub_dag_index: u64,
    ) -> bool {
        let transactions = payload
            .into_iter()
            .filter_map(|txn| {
                // Filter out transactions that wont serialize or have already been executed
                if let Ok(txn) = TransactionRequest::try_from(txn.as_ref()) {
                    if !self.query_runner.has_executed_digest(txn.hash()) {
                        Some(txn)
                    } else {
                        None
                    }
                } else {
                    None
                }
            })
            .collect::<Vec<_>>();

        let block = Block {
            digest,
            sub_dag_index,
            transactions,
        };

        let archive_block = block.clone();

        // Unfailable
        let response = self.executor.run(block).await.unwrap();
        info!("Consensus submitted new block to application");

        match self.event_tx.get() {
            Some(tx) => {
                if let Err(e) = tx
                    .send(
                        response
                            .txn_receipts
                            .iter()
                            .filter_map(|r| r.event.clone())
                            .collect(),
                    )
                    .await
                {
                    error!("We could not send a message to the RPC: {e}");
                }
            },
            None => {
                error!("Once Cell not initialized, this is a bug");
            },
        }

        let change_epoch = response.change_epoch;
        self.notifier.new_block(archive_block, response);

        if change_epoch {
            let epoch_number = self.query_runner.get_current_epoch();
            let epoch_hash = self
                .query_runner
                .get_metadata(&Metadata::LastEpochHash)
                .expect("We should have a last epoch hash")
                .maybe_hash()
                .expect("We should have gotten a hash, this is a bug");

            self.notifier.epoch_changed(epoch_number, epoch_hash);
        }

        change_epoch
    }

    pub fn shutdown(&self) {
        self.executor.downgrade();
    }

    pub fn set_event_tx(&self, tx: mpsc::Sender<Vec<Event>>) {
        self.event_tx.set(tx).unwrap();
    }
}

#[async_trait]
impl<Q: SyncQueryRunnerInterface, NE: Emitter> ExecutionState for Execution<Q, NE> {
    async fn handle_consensus_output(&self, consensus_output: ConsensusOutput) {
        let current_epoch = self.query_runner.get_current_epoch();

        let sub_dag_index = consensus_output.sub_dag.sub_dag_index;

        let batch_payload: Vec<Vec<u8>> = consensus_output
            .batches
            .into_iter()
            .filter_map(|(cert, batch)| {
                // Skip over the ones that have a different epoch. Shouldnt ever happen besides an
                // edge case towards the end of an epoch
                if cert.epoch() != current_epoch {
                    error!("we recieved a consensus cert from an epoch we are not on");
                    None
                } else {
                    // Map the batch to just the transactions
                    Some(
                        batch
                            .into_iter()
                            .flat_map(|batch| match batch {
                                // work around because batch.transactions() would require clone
                                Batch::V1(btch) => btch.transactions,
                                Batch::V2(btch) => btch.transactions,
                            })
                            .collect::<Vec<Vec<u8>>>(),
                    )
                }
            })
            .flatten()
            .collect();

        if batch_payload.is_empty() {
            return;
        }
        // We have batches in the payload send them over broadcast along with an attestion
        // of them
        let last_executed = self.query_runner.get_last_block();
        let parcel = AuthenticStampedParcel {
            transactions: batch_payload.clone(),
            last_executed,
            epoch: current_epoch,
            sub_dag_index,
        };

        let epoch_changed = self
            .submit_batch(batch_payload, parcel.to_digest(), sub_dag_index)
            .await;

        if let Err(e) = self.tx_narwhal_batches.send((parcel, epoch_changed)).await {
            // This shouldn't ever happen. But if it does there is no critical tasks
            // happening on the other end of this that would require a
            // panic
            error!("Narwhal failed to send batch payload to edge consensus: {e:?}");
        }

        // Submit the batches to application layer and if the epoch changed reset last
        // executed
        if epoch_changed {
            self.reconfigure_notify.notify_waiters();
        }
    }

    async fn last_executed_sub_dag_index(&self) -> u64 {
        // Note we add one here because of an off by 1 error in Narwhal codebase
        // if we actually return the last sub dag index that we exectuted during a restart that is
        // going to be the sub dag index they send us after a restart and we will re-execute it
        self.query_runner.get_sub_dag_index() + 1
    }
}
